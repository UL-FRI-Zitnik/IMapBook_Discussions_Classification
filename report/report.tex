% File tacl2018v2.tex
% Sep 20, 2018

% The English content of this file was modified from various *ACL instructions
% by Lillian Lee and Kristina Toutanova
%
% LaTeXery is mostly all adapted from acl2018.sty.

\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
%% Package options:
%% Short version: "hyperref" and "submission" are the defaults.
%% More verbose version:
%% Most compact command to produce a submission version with hyperref enabled
%%    \usepackage[]{tacl2018v2}
%% Most compact command to produce a "camera-ready" version
%%    \usepackage[acceptedWithA]{tacl2018v2}
%% Most compact command to produce a double-spaced copy-editor's version
%%    \usepackage[acceptedWithA,copyedit]{tacl2018v2}
%
%% If you need to disable hyperref in any of the above settings (see Section
%% "LaTeX files") in the TACL instructions), add ",nohyperref" in the square
%% brackets. (The comma is a delimiter in case there are multiple options specified.)

\usepackage[acceptedWithA]{tacl2018v2}




%%%% Material in this block is specific to generating TACL instructions
\usepackage{xspace,mfirstuc,tabulary}
\newcommand{\dateOfLastUpdate}{Sept. 20, 2018}
\newcommand{\styleFileVersion}{tacl2018v2}

\newcommand{\ex}[1]{{\sf #1}}

\newif\iftaclinstructions
\taclinstructionsfalse % AUTHORS: do NOT set this to true
\iftaclinstructions
\renewcommand{\confidential}{}
\renewcommand{\anonsubtext}{(No author info supplied here, for consistency with
TACL-submission anonymization requirements)}
\newcommand{\instr}
\fi

%
\iftaclpubformat % this "if" is set by the choice of options
\newcommand{\taclpaper}{final version\xspace}
\newcommand{\taclpapers}{final versions\xspace}
\newcommand{\Taclpaper}{Final version\xspace}
\newcommand{\Taclpapers}{Final versions\xspace}
\newcommand{\TaclPapers}{Final Versions\xspace}
\else
\newcommand{\taclpaper}{submission\xspace}
\newcommand{\taclpapers}{{\taclpaper}s\xspace}
\newcommand{\Taclpaper}{Submission\xspace}
\newcommand{\Taclpapers}{{\Taclpaper}s\xspace}
\newcommand{\TaclPapers}{Submissions\xspace}
\fi
\setcounter{secnumdepth}{4}
%%%% End TACL-instructions-specific macro block
%%%%

\title{IMapbook: Automating Analysis of Group Discussions}


% Author information does not appear in the pdf unless the "acceptedWithA" option is given
% See tacl2018v2.sty for other ways to format author information
\author{ \and }
\author{Patrik Kojanec \\ pk0404@student.uni-lj.si  
        \And
        Marko Rus \\ mr5613@student.uni-lj.si}

\date{}

\begin{document}
\maketitle

% \begin{abstract} 
% \end{abstract}

\section{Introduction}
Nature Language Processing has seen a huge rise in popularity in recent years. It is now broadly studied topic with many successful applications. In this project we touch subfield Text Classification and apply its methods to the data from IMapbook \cite{imapbook}, a web-based technology that allows reading material to be intermingled with interactive games and discussions. Some portion of discussions from this platform were manually annotated, each reply was given more categories based on the information in the reply. Our goal is to  take this data and try to build a classifier which would predict these categories. Such classifier could then be used to automate analysis of discussions at this platform, recommend the time for the teacherâ€™s intervention and more.

\subsection{Related work}
The domain of our problem is short-text classification, which is closely related to social media. Unlike the common text classification problems, where the documents are usually long and written in formal language, it deals with texts of few sentences, written in informal language. The amount of context information carried in the texts is usually very low, thus classification and information retrieval become challenging tasks to perform efficiently \cite{Song2014}. Furthermore, the low co-occurrence of words induced by the shortness of the texts often results problematic for machine learning algorithms, which rely on word frequency.\\
With the arise of social media this branch of text classification became a well researched problem, and people tried different approaches to overcome its constraints. In a survey in 2014, \cite{Song2014} pointed out the main methods of short text classification, which mostly relate on semantic analysis, since it pays more attention to the concept, inner structure semantic level, and the correlation of texts to obtain the logic structure, which is more expressive and objective. Currently, the most widely used vector representations of words (or embeddings), that proved to capture well the semantic information are GloVE \cite{glove2014} and Word2Vec \cite{word2vec}.\\
Although standard machine learning approaches often resulted problematic with short text, Sriram et al. \cite{Sriram2010} showed that their model with hand-crafted features, related to user's tweets\footnote{Short text message on the Twitter platform (www.twitter.com).}, efficiently filtered irrelevant tweets from the users, thus suggesting that by adding extra sources of context information increases the performance. Similarly, this concept was also recently shown by Yang et al. \cite{Yang2018}. Furthermore, they have also shown that Support Vector Machines performed almost equally well in classification when using word embeddings or TF-IDF, but they were outperformed by deep neural networks.\\
%Furthermore, they have also presented some examples of short text classification using ensemble classifiers, which outperformed traditional machine learning methods.\\
%\cite{Yang2018} \cite{Choudhury2015}.

\section{Dataset}
\label{dataset}
The dataset is provided by IMapBook and includes the discussions between students and teachers on the topics of the book they are reading. The dataset includes approximately 3500 Slovene messages, from 9 different schools and on 7 different books, which were also translated to English. Students in each school were divided in "book clubs", where the conversations occurred.\\
The data was manually annotated, with three main tags:
\begin{itemize}
    \item \textit{Book Relevance}: Whether the content of the message is relevant to the topic of the book discussion.
    \item \textit{Type}: Whether the message is a question (Q), answer (A) or a statement (S). In original data mixture of these classes also appear (QA and AQ), but because of their low frequency (together they appear only three times in entire dataset), we changed QA occurrences to Q and AQ to A.
    \item \textit{Category}: Whether the message is a simple chat message (C), related to the book discussion (D), moderating the discussion (M), wondering about users' identities (I), referring to a task, switching it or referring to a particular position in the application (S), or other cases (O).
\end{itemize}
The \textit{Category} category can be further on split in sub-categories; \textit{chats} may be in the form of greetings (G), related to the book (B), they could be encouraging (E), talk about feelings (F), contain cursing (C) or others (O), \textit{Discussion} messages could be questions (Q), answers (A), answers to users, still related to the discussion topic (AA) or encouraging the discussion (E); \textit{identity} messages can be answers(A), questions (Q) or their combination (QA).\\
The dataset is suitable for both binary and multi-class classification, whether the target variable is the relevance or the category of the message respectively.\\

\section{Methods}
In this section we present the methods that will be used to perform three different message classification tasks:
\begin{enumerate}
    \item Book relevance classification (binary)
    \item Type of message classification (3-class)
    \item Broad category classification (6-class)
\end{enumerate}

Input data to all classifiers (except for Majority Classifier) are questions and answers in Slovenian.

\subsection{Baseline}
As a baseline model we decided to use Majority Classifier. In each task it classifies every instance as the most representative class in training set.

\subsection{Hand-Crafted Feature Models}
The first group of models that we present is based on a hand-crafted feature set. These features were then used as an input to different classification algorithms, that we list in Section~\ref{sssc:class-alg}. We describe features extraction in the next section.

\subsubsection{Features Extraction}
The aim of the features was to simply and intuitively capture the relevance to the question, while filtering gibberish and inappropriate messages. Thus, the following set of features was designed:

\begin{itemize}
     \item Number of tokens in a message.
     \item Number of mistakes in a message; this was computed by matching words with the words in a lexicon \cite{11356/1230}.
     \item Maximal length of the token in the message.
     \item Number of characters in a message.
     \item Number of question marks in a message.
     \item Number of exclamation points in a message.
     \item Number of commas in a message.
     \item Number of periods in a message.
     \item Number of capital letters in a message.
     \item Number of capital letters within the interior of the words  in a message.
     \item Number of peculiar characters in a message.
     \item Number of numbers within the interior of the words in a message.
     \item \textit{Levenshtein distance:} Number of all pairs of words from the question and the message, whose Levenshtein distance is less than half the length of the longest of the two words.
     \item Number of interrogative words in a message.
     \item Number of "kdo" in a message.
\end{itemize}

In the case of \textit{Levenshtein distance} feature,  the messages were initially tokenized and stop-words \cite{sloStopWords} were removed, while for other cases regular expressions were used to extract the features.

All features were designed while looking at the data, having some sense in how the feature could increase the classification success. For instance, many messages  had "kdo" word in it, asking for identity of somebody. Those messages have the same class. But nevertheless we observed only small portion of the data, so that chosen features would not be overfitted.

\subsubsection{Classification Algorithms}
\label{sssc:class-alg}
We decided to feed the features to four different classification algorithms to see how they perform. We chose a na\^ive bayesian (NB), random forest (RF), support vector machine (SVM) and a logistic regression (LR) classifiers. We used the implementations from scikit-learn library \cite{scikit-learn}.

When selecting the parameters we observed train and test accuracy and paid closed attention to detecting overfitting. For NB we left the default parameters. For the SVM we used the RBF kernel and set the parameter \textit{gamma} to "auto" and \textit{C} to 5, while for the LR we decided to use "lbfgs" optimizer with maximum 1000 iterations. In the case of LR the input data was standardized to ensure equal class importances. For the RF we set the number of estimators to 150, while \textit{min\_samples\_leaf} to 3 and \textit{min\_samples\_split} to 10. This way we managed to reduce the overfitting to the training data. We kept the same parameters for all the tasks.

\section{Evaluation}
We considered more metrics for evaluation of models. Precision, recall, F1, AUC ROC metrics are defined in principle only for binary problems, at multiclass problems they loose some interpretability, so we decided not to use them. By changing the models to return probabilities, we were able to apply log loss as a metric. But the problem was that some probabilities of correct class were equal or near 0, which induced instability to the score.

So we decided to use the classification accuracy as an evaluation metric. We used 5-fold cross validation on the whole dataset, where our performance estimator was the average result of the five test sets. We also measured uncertainty, to have an opinion about variance of the estimator.

\section{Results}
Accuracies for all models and targets are shown in Figure~\ref{fig:acc_res}. We notice that all feature-based classifiers outperform the Majority Classifier. Furthermore, as expecting, the classification accuracy drops with increasing number of target classes. The best performing classification algorithm on this dataset seems to be Random Forest, which outperformed the others in both "CategoryBroad" and "Type" classification tasks. Its performance on the "Book Relevance" task was also on average higher than the rest, however SVM and LR obtained comparable results. Surprisingly, LR with multi-class extension performed quite well also on multi-class classification tasks.

Initially, RF yielded very high performance on the training set, reaching a 95\% accuracy. However, the performance on the test set was lower, showing signs of overfitting. Thus, with a more careful selection of the parameters, we dropped the training accuracy for about 10\% and reached the current test performance.

\begin{figure}[tbh]
    \centering
    \includegraphics[width = \linewidth]{../results/Accuracy_plot.pdf}
    \caption{\textbf{Accuracies.} Classification accuracies of models on three different classification tasks described in Section~\ref{dataset}. Random Forest achieves 80.4\%, 71.5\% and 65.4\% accuracies on "Book relevance", "Type" and "CategoryBroad" targets respectively.}
    \label{fig:acc_res}
\end{figure}

\subsection{Features Importance}
RF is often used as a features selection tool, as it ranks the importance of the features. In Figure \ref{fig:ft_imp_RF} we show the importance of each feature in the decision process of the RF model.

\begin{figure}[tbh]
    \centering
    \includegraphics[width = \linewidth]{../results/features_imp_RF_plot.pdf}
    \caption{\textbf{Features Importance.} \textit{Lev. distance} between answer and question, general length of message and number of mistakes show as important features.}
    \label{fig:ft_imp_RF}
\end{figure}

As we notice, each classification task focuses on different features, however there are some common ones that are discriminatory for all three tasks, i.e. last five in the plot. As expected, \textit{Lev. distance} works particularly well on the "Book relevance" problem, since it performs a naive kind of matching of the text messages with the questions. However, it results also as the most discriminatory feature for "Type" classification and third for "CategoryBroad" classification.

It is not surprising that some features are particularly relevant to some classification tasks, since they were designed for that purpose. It is also known that good features increase performance. Here we showed that some features are particularly suitable for some specific tasks, while others behave well over different classification problems. One future improvement that could be done is trying to define some other features that would boost the performance, removing the irrelevant ones.

\section{Future Perspective}
Random forest seems to be a very powerful classification algorithm that fits well on the given data. However, features given to it were only simple properties of the messages, extracted by hand. 

What we would like to try is to create embeddings not by hand, but with some more sophisticated model, as Neural Networks. Many pretrained Neural Networks exist (BERT \cite{devlin2018bert}), which can be used to extract contextual embeddings. 




% \begin{table}[t]
% \begin{center}
% \begin{tabular}{|l|rl|}
% \hline \bf Type of Text & \bf Size & \bf Style \\ \hline
% paper title & 15 pt & bold \\
% \iftaclpubformat
% author names & 12 pt & bold \\
% author affiliation & 12 pt & \\
% \else
% \fi
% the word ``Abstract'' as header & 12 pt & bold \\
% abstract text & 10 pt & \\
% section titles & 12 pt & bold \\
% document text & 11 pt  &\\
% captions & 10 pt & \\
% %bibliography & 10 pt & \\
% footnotes & 9 pt & \\
% \hline
% \end{tabular}
% \end{center}
% \caption{\label{tab:font-table} Font requirements}
% \end{table}


\bibliography{tacl2018}
\bibliographystyle{acl_natbib}

\end{document}


