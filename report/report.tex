% File tacl2018v2.tex
% Sep 20, 2018

% The English content of this file was modified from various *ACL instructions
% by Lillian Lee and Kristina Toutanova
%
% LaTeXery is mostly all adapted from acl2018.sty.

\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
%% Package options:
%% Short version: "hyperref" and "submission" are the defaults.
%% More verbose version:
%% Most compact command to produce a submission version with hyperref enabled
%%    \usepackage[]{tacl2018v2}
%% Most compact command to produce a "camera-ready" version
%%    \usepackage[acceptedWithA]{tacl2018v2}
%% Most compact command to produce a double-spaced copy-editor's version
%%    \usepackage[acceptedWithA,copyedit]{tacl2018v2}
%
%% If you need to disable hyperref in any of the above settings (see Section
%% "LaTeX files") in the TACL instructions), add ",nohyperref" in the square
%% brackets. (The comma is a delimiter in case there are multiple options specified.)

\usepackage[acceptedWithA]{tacl2018v2}




%%%% Material in this block is specific to generating TACL instructions
\usepackage{xspace,mfirstuc,tabulary}
\newcommand{\dateOfLastUpdate}{Sept. 20, 2018}
\newcommand{\styleFileVersion}{tacl2018v2}

\newcommand{\ex}[1]{{\sf #1}}

\newif\iftaclinstructions
\taclinstructionsfalse % AUTHORS: do NOT set this to true
\iftaclinstructions
\renewcommand{\confidential}{}
\renewcommand{\anonsubtext}{(No author info supplied here, for consistency with
TACL-submission anonymization requirements)}
\newcommand{\instr}
\fi

%
\iftaclpubformat % this "if" is set by the choice of options
\newcommand{\taclpaper}{final version\xspace}
\newcommand{\taclpapers}{final versions\xspace}
\newcommand{\Taclpaper}{Final version\xspace}
\newcommand{\Taclpapers}{Final versions\xspace}
\newcommand{\TaclPapers}{Final Versions\xspace}
\else
\newcommand{\taclpaper}{submission\xspace}
\newcommand{\taclpapers}{{\taclpaper}s\xspace}
\newcommand{\Taclpaper}{Submission\xspace}
\newcommand{\Taclpapers}{{\Taclpaper}s\xspace}
\newcommand{\TaclPapers}{Submissions\xspace}
\fi
\setcounter{secnumdepth}{4}
%%%% End TACL-instructions-specific macro block
%%%%

\title{IMapbook: Automating Analysis of Group Discussions}


% Author information does not appear in the pdf unless the "acceptedWithA" option is given
% See tacl2018v2.sty for other ways to format author information
\author{ \and }
\author{Patrik Kojanec \\ pk0404@student.uni-lj.si  \\ Univerza v Ljubljani,\\ Ve\v cna pot 113,\\ 1000 Ljubljana
        \And
        Marko Rus \\ mr5613@student.uni-lj.si\\ Univerza v Ljubljani,\\ Ve\v cna pot 113,\\ 1000 Ljubljana}

\date{}

\begin{document}
\maketitle

\begin{abstract} 
The constant advancements in technology introduce new approaches and tools for students' education. Such tool is the ImapBook, an interactive platform that allows students to communicate and discuss about the book they are reading. To give the best support, such applications need natural language processing features, that understand the content of communications and automatically intervene when the focus on the topic is lost. Thus, we present different text classification models that classify such texts from students' communications into useful categories. The best performing method is fine-tuned BERT, which outperforms simpler classification methods based on handcrafted features.
\end{abstract}

\section{Introduction}
Nature Language Processing has seen a huge rise in popularity in recent years. It is now broadly studied topic with many successful applications. In this project we touch subfield Text Classification and apply its methods to the data from IMapbook \cite{imapbook}, a web-based technology that allows reading material to be intermingled with interactive games and discussions. Some portion of discussions from this platform were manually annotated, each reply was given more categories based on the information in the reply. Our goal is to  take this data and try to build a classifier which would predict these categories. Such classifier could then be used to automate analysis of discussions at this platform, recommend the time for the teacher’s intervention and more.

\subsection{Related work}
The domain of our problem is short-text classification, which is closely related to social media. Unlike the common text classification problems, where the documents are usually long and written in formal language, it deals with texts of few sentences, written in informal language. The amount of context information carried in the texts is usually very low, thus classification and information retrieval become challenging tasks to perform efficiently \cite{Song2014}. Furthermore, the low co-occurrence of words induced by the shortness of the texts often results problematic for machine learning algorithms, which rely on word frequency.\\
With the arise of social media this branch of text classification became a well researched problem, and people tried different approaches to overcome its constraints. In a survey in 2014, \cite{Song2014} pointed out the main methods of short text classification, which mostly relate on semantic analysis, since it pays more attention to the concept, inner structure semantic level, and the correlation of texts to obtain the logic structure, which is more expressive and objective. Currently, the most widely used vector representations of words (or embeddings), that proved to capture well the semantic information are GloVE \cite{glove2014} and Word2Vec \cite{word2vec}.\\
Although standard machine learning approaches often resulted problematic with short text, Sriram et al. \cite{Sriram2010} showed that their model with hand-crafted features, related to user's tweets\footnote{Short text message on the Twitter platform (www.twitter.com).}, efficiently filtered irrelevant tweets from the users, thus suggesting that by adding extra sources of context information increases the performance. Similarly, this concept was also recently shown by Yang et al. \cite{Yang2018}. Furthermore, they have also shown that Support Vector Machines performed almost equally well in classification when using word embeddings or TF-IDF, but they were outperformed by deep neural networks.\\
%Furthermore, they have also presented some examples of short text classification using ensemble classifiers, which outperformed traditional machine learning methods.\\
%\cite{Yang2018} \cite{Choudhury2015}.

\section{Dataset}
\label{dataset}
The dataset is provided by IMapBook and includes the discussions between students and teachers on the topics of the book they are reading. The dataset includes approximately 3500 Slovene messages, from 9 different schools and on 7 different books, which were also translated to English. Students in each school were divided in "book clubs", where the conversations occurred.\\
The data was manually annotated, with three main tags:
\begin{itemize}
    \item \textit{Book Relevance}: Whether the content of the message is relevant to the topic of the book discussion.
    \item \textit{Type}: Whether the message is a question (Q), answer (A) or a statement (S). In original data mixture of these classes also appear (QA and AQ), but because of their low frequency (together they appear only three times in entire dataset), we changed QA occurrences to Q and AQ to A.
    \item \textit{Category}: Whether the message is a simple chat message (C), related to the book discussion (D), moderating the discussion (M), wondering about users' identities (I), referring to a task, switching it or referring to a particular position in the application (S), or other cases (O).
\end{itemize}
The \textit{Category} category can be further on split in sub-categories; \textit{chats} may be in the form of greetings (G), related to the book (B), they could be encouraging (E), talk about feelings (F), contain cursing (C) or others (O), \textit{Discussion} messages could be questions (Q), answers (A), answers to users, still related to the discussion topic (AA) or encouraging the discussion (E); \textit{identity} messages can be answers(A), questions (Q) or their combination (QA).\\
The dataset is suitable for both binary and multi-class classification, whether the target variable is the relevance or the category of the message respectively.\\

\section{Methods}
In this section we present the methods that will be used to perform three different message classification tasks:
\begin{enumerate}
    \item Book relevance classification (binary)
    \item Type of message classification (3-class)
    \item Broad category classification (6-class)
\end{enumerate}

Input data to all classifiers are exchanged messages. To provide information about wether users are discussing about relevant topic, each message also has information about the question provided to users before the discussion.

\subsection{Baseline}
As a baseline model we decided to use Majority Classifier. In each task it classifies every instance as the most representative class in training set.

\subsection{Hand-Crafted Feature Models}
The first group of models that we present is based on a hand-crafted feature set. These features were then used as an input to different classification algorithms, that we list in Section~\ref{sssc:class-alg}. We describe features extraction in the next section.

\subsubsection{Features Extraction}
The aim of the features was to simply and intuitively capture the relevance to the question, while filtering gibberish and inappropriate messages. Thus, the following set of features was designed:

\begin{itemize}
     \item Number of tokens in a message.
     \item Number of mistakes in a message; this was computed by matching words with the words in a lexicon \cite{11356/1230}.
     \item Maximal length of the token in the message.
     \item Number of characters in a message.
     \item Number of question marks in a message.
     \item Number of exclamation points in a message.
     \item Number of commas in a message.
     \item Number of periods in a message.
     \item Number of capital letters in a message.
     \item Number of capital letters within the interior of the words  in a message.
     \item Number of peculiar characters in a message.
     \item Number of numbers within the interior of the words in a message.
     \item \textit{Levenshtein distance:} Number of all pairs of words from the question and the message, whose Levenshtein distance is less than half the length of the longest of the two words.
     \item Number of interrogative words in a message.
     \item Number of "kdo" in a message.
\end{itemize}

In the case of \textit{Levenshtein distance} feature,  the messages were initially tokenized and stop-words \cite{sloStopWords} were removed, while for other cases regular expressions were used to extract the features.

All features were designed while looking at the data, having some sense in how the feature could increase the classification success. For instance, many messages  had "kdo" word in it, asking for identity of somebody. Those messages have the same class. But nevertheless we observed only small portion of the data, so that chosen features would not be overfitted.

\subsubsection{Classification Algorithms}
\label{sssc:class-alg}
We decided to feed the features to four different classification algorithms to see how they perform. We chose a na\^ive bayesian (NB), random forest (RF), support vector machine (SVM) and a logistic regression (LR) classifiers. We used the implementations from scikit-learn library \cite{scikit-learn}.

When selecting the parameters we observed train and test accuracy and paid closed attention to detecting overfitting. For NB we left the default parameters. For the SVM we used the RBF kernel and set the parameter \textit{gamma} to "auto" and \textit{C} to 5, while for the LR we decided to use "lbfgs" optimizer with maximum 1000 iterations. In the case of LR the input data was standardized to ensure equal class importance. For the RF we set the number of estimators to 150, while \textit{min\_samples\_leaf} to 3 and \textit{min\_samples\_split} to 10. This way we managed to reduce the overfitting to the training data. We kept the same parameters for all the tasks.

\subsection{ELMo Embeddings}
We handcrafted features by looking at the messages and observed what could potentially discriminate different types of messages. For the next experiment we wanted to know, how good features can we extract automatically, so that such human interaction and understanding of messages wouldn't be necessary.

ELMo~\cite{peters2018deep} is model for creating contextual embeddings. We have chosen it as it can also be used to embed entire message. Firstly we put discussion topic into it, and then message, so that message's embedding also contains information about the relevance to the topic.

We have used pretrained ELMo model for Slovene language~\cite{elmosl}. 

For classification we tried all models discussed in~\ref{sssc:class-alg} and also KNN~\cite{fukunaga1975branch} with cosine distance, as it is natural distance to use in ELMo embeddings. Random Forest classifier ended up having the highest performance.

\subsubsection{Fixing Typos in Messages}
Messages in the input data contain a lot of words, that have typos in them and are not part of the Slovene lexicon~\cite{11356/1230}. Also, a lot of mistakes come from users deliberately leaving out carrot (e.g. 's' instead of 'š'). That is why we decided to write an algorithm for correcting typos that are away from the correct word for at most Levenshtein distance of two. We also calculated probabilities of the words and removed words with probability less than $10^{-8}$. 

\subsection{BERT Fine-Tuning}
An other approach we propose is using a pretrained BERT \cite{devlin2018bert}, by fine-tuning it for our classification tasks. We avoided customizing BERT models, because they require notorious amount of data which was not available. We trained our BERT model for Slovenian, Croatian and English languages for sequence classification for three epochs on training data that consisted of 80\% of our dataset, while the remaining 20\% was left for testing. Out of these 80\%, 15\% were used for validation. We trained one model for each task, for both Slovenian and English-translated messages.

\section{Evaluation}
We evaluated the models using F1 evaluation metric. At multi-class problems we used weighting over different classes to compute it. 

Because of the complexity of our models, we opted for two different evaluation techniques: on models that are not so computationally expensive to fit, we used 5-fold cross validation on the whole dataset, where our performance estimator was the average result of the five test sets. This technique also points out the variance of our estimator, hence quantifying to some extend the uncertainty of the performances of our models. The second evaluation is a simple hold-out evaluation, where we split train and test sets at 80\%, thus losing information about the variability of the performance of our predictor.

\section{Results and Discussion}
\subsection{Baseline Models}
Scores for computationally less expensive models (Majority Classifier and different models with handcrafted features) are shown in Figure~\ref{fig:acc_res}. We notice that all feature-based classifiers outperform the Majority Classifier. Furthermore, as expecting, the classification accuracy drops with increasing number of target classes. The best performing classification algorithm on this dataset is Random Forest, which outperformed the others in both "CategoryBroad" and "Type" classification tasks. Its performance on the "Book Relevance" task was also on average higher than the rest, however SVM and LR obtained comparable results.

Initially, RF yielded very high performance on the training set, reaching a 95\% accuracy. However, the performance on the test set was lower, showing signs of overfitting. Thus, with a more careful selection of the parameters, we dropped the training accuracy for about 10\% and reached the current test performance.

\begin{figure}[tbh]
    \centering
    \includegraphics[width = \linewidth]{../results/plot_baselines.pdf}
    \caption{\textbf{F1 scores.} Scores of baseline models on three different classification tasks described in Section~\ref{dataset}.}
    \label{fig:acc_res}
\end{figure}

\subsubsection{Features Importance}
RF is often used as a features selection tool, as it ranks the importance of the features. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features. I In Figure \ref{fig:ft_imp_RF} we show the importance of each feature in the decision process of the RF model.

\begin{figure}[tbh]
    \centering
    \includegraphics[width = \linewidth]{../results/features_imp_RF_plot.pdf}
    \caption{\textbf{Features Importance.} \textit{Lev. distance} between answer and question, general length of message, and number of mistakes show as important features.}
    \label{fig:ft_imp_RF}
\end{figure}

As we notice, each classification task focuses on different features, however there are some common ones that are discriminatory for all three tasks, i.e. last five in the plot. As expected, \textit{Lev. distance} works particularly well on the "Book relevance" problem, since it performs a naïve kind of matching of the text messages with the questions. However, it results also as the most discriminatory feature for "Type" classification and third for "CategoryBroad" classification.

It is not surprising that some features are particularly relevant to some classification tasks, since they were designed for that purpose. It is also known that good features increase performance. Here we showed that some features are particularly suitable for some specific tasks, while others behave well over different classification problems. One future improvement that could be done is trying to define some other features that would boost the performance, removing the irrelevant ones.

\subsection{Deep Models}
\begin{figure}[tbh]
    \centering
    \includegraphics[width = \linewidth]{../results/plot_deep_models.pdf}
    \caption{\textbf{Hold-out performance evaluation.} Comparing performances on the test set of BERT, Handcrafted Features Model and ELMo model.}
    \label{fig:h-o_eval}
\end{figure}

F1 scores from hold-out evaluation can be seen in the Figure~\ref{fig:h-o_eval} and in the following table.

\begin{center}
\begin{tabular}{ c|c c c }
     & Relevance & Type & Categ. \\ 
    \hline
    Handcrafted & 0.78 & 0.70 & 0.66 \\ 
    ELMo & 0.75 & 0.63 & 0.58 \\ 
    BERT & 0.90 & \textbf{0.85} & 0.78 \\ 
    BERT (Eng) & \textbf{0.91} & 0.85 & \textbf{0.85} \\ 
\end{tabular}
\end{center}

Here BERT (Eng) is BERT trained on English translations. Note that these translations were made by human and wouldn't be present in unseen data.

\subsubsection{ELMo}

We can see that ELMo has worse performance than baseline model with handcrafted features. But it is important to note here, that handcrafted features may be overfitted to the given data. If model was applied to discussions from older children, same features may perform worse. In the other hand, ELMo features are generated automatically and may generalize better.

\subsubsection{BERT}
When analyzing performance of the BERT models, we can clearly see 15 - 20\% increase in performance compared to model with handcrafted features. BERT model that uses English translations is even more successful, especially in the classification of the category, where we can observe nearly 29\% increase in performance. This clearly demonstrates dominance of BERT models.

We would like to mention, that here we did not measure uncertainty of the scores. But scores are still comparable, as we evaluated models on the same test set.
 
\subsection{Analyzing Predictions}
A lot of messages are asking for identity of somebody, and such messages were mostly successfully classified by all models. Lots of messages contain a lot of gibberish and are as such distinguishable from other messages. Harder to predict are messages that are short and contain only few words. Models performed worse also on messages with a lot of unidentified mistakes in words. 

\section{Conclusion}
The main question that we were answering was to what extent can messages be automatically recognized as of certain type. We showed that you can achieve decent performance with handcrafted features and simple models, as also with fully automatic generation of features. Furthermore, we fine-tuned end-to-end BERT neural network, yielding a significant increase in performance.

\subsection{Further Work}
Messages typically have a flow, messages closer together are more probable to have the same type. In our approach we discarded this information about the time the message was sent, which resulted in the loss of information. One possible improvement to our models would be to somehow incorporate this information to the models, to identify messages that are a direct reply to some other message. This would improve performance at short messages, which are without such context rarely meaningful.

 

\bibliography{tacl2018}
\bibliographystyle{acl_natbib}

\end{document}


